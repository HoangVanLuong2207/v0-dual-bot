import { type NextRequest, NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;
const GOOGLE_API_KEY = process.env.GOOGLE_API_KEY;
const TAVILY_API_KEY = process.env.TAVILY_API_KEY;

const OPENAI_BASE_URL = "https://api.openai.com/v1";
const GOOGLE_BASE_URL = "https://generativelanguage.googleapis.com/v1beta";

// Model mapping to determine which API to use
const MODEL_MAPPING = {
  // OpenAI models
  "gpt-4o": { provider: "openai", model: "gpt-4o" },
  "gpt-4o-mini": { provider: "openai", model: "gpt-4o-mini" },
  "gpt-4-turbo": { provider: "openai", model: "gpt-4-turbo" },
  "gpt-3.5-turbo": { provider: "openai", model: "gpt-3.5-turbo" },

  // Google models - updated with newer versions
  "gemini-2.5-pro": { provider: "google", model: "gemini-2.5-pro" },
  "gemini-2.5-flash": { provider: "google", model: "gemini-2.5-flash" },
  "gemini-2.5-flash-lite": { provider: "google", model: "gemini-2.5-flash-lite" },
  "gemini-2.0-flash": { provider: "google", model: "gemini-2.0-flash" },
  "gemini-2.0-flash-lite": { provider: "google", model: "gemini-2.0-flash-lite" },
  "gemini-2.0-pro": { provider: "google", model: "gemini-2.0-pro" },
  "gemini-1.5-flash": { provider: "google", model: "gemini-1.5-flash" },
  "gemini-1.5-flash-lite": { provider: "google", model: "gemini-1.5-flash-lite" },
  "gemini-1.5-pro": { provider: "google", model: "gemini-1.5-pro" },
  "gemini-1.0-pro": { provider: "google", model: "gemini-1.0-pro" },
  "gemini-1.0-ultra": { provider: "google", model: "gemini-1.0-ultra" },
  "gemini-1.0-nano": { provider: "google", model: "gemini-1.0-nano" },
  "gemini-embedding": { provider: "google", model: "gemini-embedding" },
  "gemini-2.5-flash-preview-tts": { provider: "google", model: "gemini-2.5-flash-preview-tts" },
  "gemini-2.5-flash-preview-image-generation": { provider: "google", model: "gemini-2.5-flash-preview-image-generation" },
  "gemini-2.5-flash-live-preview-04-09": { provider: "google", model: "gemini-2.5-flash-live-preview-04-09" },
};

const genAI = GOOGLE_API_KEY ? new GoogleGenerativeAI(GOOGLE_API_KEY) : null;

// Tavily search function
async function searchWithTavily(query: string) {
  if (!TAVILY_API_KEY) {
    console.warn("Tavily API key not configured, skipping search");
    return null;
  }

  try {
    console.log("[Tavily] Searching for:", query.substring(0, 100) + "...");

    const response = await fetch("https://api.tavily.com/search", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        api_key: TAVILY_API_KEY,
        query: query,
        search_depth: "advanced",
        include_answer: true,
        include_raw_content: false,
        max_results: 5,
        include_domains: [],
        exclude_domains: [],
      }),
    });

    if (!response.ok) {
      console.error("Tavily API error:", response.status, response.statusText);
      return null;
    }

    const data = await response.json();
    console.log("[Tavily] Search completed:", {
      resultsCount: data.results?.length || 0,
      hasAnswer: !!data.answer,
    });

    return data;
  } catch (error) {
    console.error("Tavily search error:", error);
    return null;
  }
}

export async function POST(request: NextRequest) {
  try {
    const {
      messages,
      model,
      stream = false,
      files = [],
      workflow = "single",
    } = await request.json();

    const modelConfig = MODEL_MAPPING[model as keyof typeof MODEL_MAPPING];
    if (!modelConfig) {
      return NextResponse.json(
        { error: `Unsupported model: ${model}` },
        { status: 400 }
      );
    }

    const { provider, model: actualModel } = modelConfig;

    // Workflow: tavily-to-gemini (search with Tavily, then ask Gemini)
    if (workflow === "tavily-to-gemini") {
      return await handleTavilyToGemini(messages, actualModel, stream, files);
    }

    // Default: direct Gemini processing
    return await handleGoogle(messages, actualModel, stream, files);
  } catch (error) {
    console.error("Direct Chat API error:", error);
    return NextResponse.json(
      { error: "Internal server error" },
      { status: 500 }
    );
  }
}

// Tavily-to-Gemini workflow handler
async function handleTavilyToGemini(
  messages: any[],
  model: string,
  stream: boolean,
  files: any[]
) {
  if (!GOOGLE_API_KEY || !genAI) {
    return NextResponse.json(
      { error: "Google API key not configured" },
      { status: 500 }
    );
  }

  try {
    // Step 1: Extract user question
    const lastMessage = messages[messages.length - 1];
    const userQuestion = 
      typeof lastMessage.content === "string"
        ? lastMessage.content
        : lastMessage.content?.find((c: any) => c.type === "text")?.text || "";

    if (!userQuestion.trim()) {
      return NextResponse.json(
        { error: "No question provided" },
        { status: 400 }
      );
    }

    // Step 2: Search with Tavily
    console.log("[Tavily-to-Gemini] Step 1: Searching with Tavily");
    const searchResults = await searchWithTavily(userQuestion);

    // Step 3: Build enhanced prompt with search results
    let enhancedPrompt = userQuestion;
    
    if (searchResults && searchResults.results && searchResults.results.length > 0) {
      const searchContext = searchResults.results
        .map((result: any, index: number) => 
          `${index + 1}. **${result.title}**\n${result.content}\nüìç Ngu·ªìn: ${result.url}`
        )
        .join("\n\n");

      const tavilyAnswer = searchResults.answer 
        ? `\n**T√≥m t·∫Øt t·ª´ Tavily AI:**\n${searchResults.answer}\n\n` 
        : "";

      enhancedPrompt = `${tavilyAnswer}D·ª±a tr√™n th√¥ng tin t√¨m ki·∫øm m·ªõi nh·∫•t sau ƒë√¢y, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa t√¥i m·ªôt c√°ch chi ti·∫øt v√† ch√≠nh x√°c:

üìö **TH√îNG TIN T√åM KI·∫æM:**
${searchContext}

‚ùì **C√ÇU H·ªéI:** ${userQuestion}

üìã **Y√äU C·∫¶U TR·∫¢ L·ªúI:**
- B·∫°n ƒë√≥ng vai tr√≤ l√† m·ªôt chuy√™n gia trong lƒ©nh v·ª±c t√¨m ki·∫øm t√†i li·ªáu d·ª±a theo y√™u c·∫ßu ${userQuestion}
- T·ª´ t√†i li·ªáu thu th·∫≠p ƒë∆∞·ª£c, b·∫°n s·∫Ω t·ªïng h·ª£p l·∫°i th√†nh m·ªôt prompt ƒë·ªÉ g·ª≠i cho Gemini
- Kh√¥ng d√πng k√Ω hi·ªáu **, kh√¥ng markdown
- N·∫øu th√¥ng tin t√¨m ki·∫øm kh√¥ng ƒë·ªß ho·∫∑c m√¢u thu·∫´n, h√£y n√≥i r√µ
- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, s·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng d·ªÖ ƒë·ªçc v·ªõi bullet points khi c·∫ßn thi·∫øt
- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát`;
    }

    // Step 4: Process messages for Gemini with enhanced prompt
    const processedMessages = [...messages];
    processedMessages[processedMessages.length - 1] = {
      ...processedMessages[processedMessages.length - 1],
      content: enhancedPrompt
    };

    // Step 5: Call Gemini
    console.log("[Tavily-to-Gemini] Step 2: Calling Gemini");
    const geminiModel = genAI.getGenerativeModel({ model });
    
    const result = await geminiModel.generateContent({
      contents: processedMessages.map(msg => ({
        role: msg.role === "assistant" ? "model" : "user",
        parts: [{ text: typeof msg.content === "string" ? msg.content : JSON.stringify(msg.content) }]
      })),
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 2000,
      },
    });

    const response = await result.response;
    const text = response.text();

    console.log("[Tavily-to-Gemini] Completed:", {
      textLength: text.length,
      searchResultsCount: searchResults?.results?.length || 0,
    });

    // Step 6: Return response with metadata
    return NextResponse.json({
      candidates: [
        {
          content: {
            parts: [{ text }],
            role: "model",
          },
          finishReason: "STOP",
        },
      ],
      choices: [
        {
          message: {
            role: "assistant",
            content: text,
          },
        },
      ],
      searchResults,
      workflow: "tavily-to-gemini",
      step1: {
        type: "tavily_search",
        query: userQuestion,
        resultsCount: searchResults?.results?.length || 0,
      },
      step2: {
        type: "gemini_response",
        promptLength: enhancedPrompt.length,
      },
    });

  } catch (error: any) {
    console.error("Tavily-to-Gemini error:", error);
    return NextResponse.json(
      {
        error: error?.message || "Failed to process Tavily-to-Gemini workflow",
        errorType: "workflow_error",
      },
      { status: 500 }
    );
  }
}

async function handleOpenAI(
  messages: any[],
  model: string,
  stream: boolean,
  files: any[]
) {
  if (!OPENAI_API_KEY) {
    return NextResponse.json(
      { error: "OpenAI API key not configured" },
      { status: 500 }
    );
  }

  // Process files for OpenAI (supports images, not PDFs directly)
  const processedMessages = await processMessagesForOpenAI(messages, files);

  const requestBody = {
    model,
    messages: processedMessages,
    stream,
    temperature: 0.7,
    max_tokens: 2000,
  };

  console.log("Sending request to OpenAI:", {
    model,
    messagesCount: processedMessages.length,
    filesCount: files.length,
  });

  try {
    const response = await fetch(`${OPENAI_BASE_URL}/chat/completions`, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${OPENAI_API_KEY}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(requestBody),
    });

    if (!response.ok) {
      const errorData = await response.json();
      console.error("OpenAI API error:", errorData);

      if (errorData.error?.code === "insufficient_quota") {
        return NextResponse.json(
          {
            error:
              "OpenAI quota exceeded. Please check your billing or try using Gemini instead.",
            errorType: "quota_exceeded",
            fallbackSuggestion: "gemini",
          },
          { status: 402 }
        );
      }

      if (errorData.error?.code === "invalid_api_key") {
        return NextResponse.json(
          {
            error: "Invalid OpenAI API key. Please check your configuration.",
            errorType: "invalid_key",
          },
          { status: 401 }
        );
      }

      return NextResponse.json(
        {
          error:
            errorData.error?.message || "Failed to get response from OpenAI",
          errorType: "api_error",
        },
        { status: response.status }
      );
    }

    if (stream) {
      return new Response(response.body, {
        headers: {
          "Content-Type": "text/event-stream",
          "Cache-Control": "no-cache",
          Connection: "keep-alive",
        },
      });
    } else {
      const data = await response.json();
      return NextResponse.json(data);
    }
  } catch (error) {
    console.error("OpenAI request failed:", error);
    return NextResponse.json(
      {
        error: "Network error when calling OpenAI API",
        errorType: "network_error",
      },
      { status: 500 }
    );
  }
}

async function handleGoogle(
  messages: any[],
  model: string,
  stream: boolean,
  files: any[]
) {
  if (!GOOGLE_API_KEY || !genAI) {
    return NextResponse.json(
      { error: "Google API key not configured" },
      { status: 500 }
    );
  }

  try {
    console.log("messages üëâ", messages, files);
    const uploadedFiles = await uploadFilesToGeminiSDK(files);

    const { processedContents, prompts } = await processMessagesForGoogleSDK(
      messages,
      files,
      uploadedFiles
    );

    console.log("Sending request to Google Gemini SDK:", {
      model,
      contentsCount: processedContents.length,
      filesCount: files.length,
      uploadedFilesCount: uploadedFiles.length,
    });

    // 1. Khai b√°o c√¥ng c·ª• b·∫°n mu·ªën s·ª≠ d·ª•ng
    const tools = [
      {
        googleSearchRetrieval: {}, // C√∫ ph√°p cho Google Search trong Node.js/JS
      },
    ];

    const geminiModel = genAI.getGenerativeModel({ model });

    const result = await geminiModel.generateContent({
      contents: processedContents,
      generationConfig: {
        temperature: 0.7,
        maxOutputTokens: 1000,
      },
    });

    const response = await result.response;
    const text = response.text();

    console.log("Google Gemini SDK response received:", {
      textLength: text.length,
      textPreview: text.substring(0, 200) + "...",
    });

    return NextResponse.json({
      candidates: [
        {
          content: {
            parts: [{ text }],
            role: "model",
          },
          finishReason: "STOP",
        },
      ],
      usageMetadata: response.usageMetadata,
      // Keep compatibility with frontend
      choices: [
        {
          message: {
            role: "assistant",
            content: text,
          },
        },
      ],
      prompts,
    });
  } catch (error: any) {
    console.error("Google GenAI SDK error:", error);

    if (error?.message?.includes("API key")) {
      return NextResponse.json(
        {
          error: "Invalid Google API key. Please check your configuration.",
          errorType: "invalid_key",
        },
        { status: 401 }
      );
    }

    if (error?.message?.includes("quota") || error?.message?.includes("limit")) {
      return NextResponse.json(
        {
          error: "Google API quota exceeded. Please try again later.",
          errorType: "quota_exceeded",
        },
        { status: 429 }
      );
    }

    return NextResponse.json(
      {
        error: error?.message || "Failed to get response from Google Gemini",
        errorType: "api_error",
      },
      { status: 500 }
    );
  }
}

async function uploadFilesToGeminiSDK(files: any[]) {
  if (!genAI) return [];

  const uploadedFiles = [];

  for (const file of files) {
    // Upload Office files and PDFs via Files API
    const needsUpload = [
      "application/vnd.openxmlformats-officedocument.wordprocessingml.document", // .docx
      "application/msword", // .doc
      "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", // .xlsx
      "application/vnd.ms-excel", // .xls
      "application/vnd.openxmlformats-officedocument.presentationml.presentation", // .pptx
      "application/pdf", // PDF files
    ].includes(file.type);

    if (needsUpload) {
      try {
        console.log(`Uploading ${file.name} to Gemini Files API using SDK...`);

        const buffer = Buffer.from(file.data, "base64");

        // @ts-ignore - files API may not be typed in all SDK versions
        const uploadResult = await (genAI as any).files.upload({
          file: buffer,
          mimeType: file.type,
          displayName: file.name,
        });

        console.log(`Successfully uploaded ${file.name}:`, {
          uri: uploadResult.file.uri,
          name: uploadResult.file.name,
          mimeType: uploadResult.file.mimeType,
        });

        uploadedFiles.push({
          originalFile: file,
          fileUri: uploadResult.file.uri,
          name: uploadResult.file.name,
          mimeType: uploadResult.file.mimeType,
        });
      } catch (error) {
        console.error(`Error uploading ${file.name}:`, error);
      }
    }
  }

  return uploadedFiles;
}

// --- Reusable: chu·∫©n ho√° + th√™m/s·ª≠a message + upload & attach files ---
async function prepareMessagesForGemini(
  messages: any[],
  files: any[],
  {
    systemPrompt,
    replaceLastUser,
    attachFiles = true,
    inlineFallback = true,
  }: {
    systemPrompt?: string; // s·∫Ω tr·∫£ ra qua field systemInstruction (KH√îNG th√™m v√†o contents)
    replaceLastUser?: string;
    attachFiles?: boolean;
    inlineFallback?: boolean;
  } = {}
) {
  const normalizeMessage = (msg: any) => {
    // Lo·∫°i b·ªè m·ªçi message role=system trong contents ƒë·ªÉ tr√°nh 400
    if (msg?.role === "system") return null;

    if (msg?.parts && Array.isArray(msg.parts)) return msg;

    if (typeof msg?.content === "string") {
      return { role: msg.role, parts: [{ text: msg.content }] };
    }

    if (Array.isArray(msg?.content)) {
      const parts = msg.content
        .map((c: any) => {
          if (c?.type === "text" && typeof c.text === "string")
            return { text: c.text };
          if (c?.inlineData?.data && c?.inlineData?.mimeType)
            return {
              inlineData: {
                data: c.inlineData.data,
                mimeType: c.inlineData.mimeType,
              },
            };
          if (c?.fileData?.fileUri && c?.fileData?.mimeType)
            return {
              fileData: {
                fileUri: c.fileData.fileUri,
                mimeType: c.fileData.mimeType,
              },
            };
          return null;
        })
        .filter(Boolean);
      return { role: msg.role, parts };
    }

    return { role: msg.role, parts: [] };
  };

  // 1) Normalize & lo·∫°i system kh·ªèi contents
  let arr = (messages || []).map(normalizeMessage).filter(Boolean);

  // 2) Optional: replace last user
  if (replaceLastUser) {
    for (let i = arr.length - 1; i >= 0; i--) {
      if (arr[i].role === "user") {
        arr[i] = { ...arr[i], parts: [{ text: replaceLastUser }] };
        break;
      }
    }
  }

  // 3) Upload & attach files v√†o last user
  let uploadedFiles: any[] = [];
  let fileParts: any[] = [];
  if (attachFiles && files?.length) {
    try {
      uploadedFiles = await uploadFilesToGeminiSDK(files);
      if (uploadedFiles?.length) {
        fileParts = uploadedFiles
          .map((f: any) => {
            const uri = f?.fileUri || f?.uri || f?.name; // "files/xxx" c≈©ng OK
            const mime = f?.mimeType;
            if (!uri || !mime) return null;
            return { fileData: { fileUri: uri, mimeType: mime } };
          })
          .filter(Boolean);
      }
    } catch (e) {
      console.error(
        "prepareMessagesForGemini: upload error, fallback inline if enabled",
        e
      );
    }

    if (!fileParts.length && inlineFallback) {
      const inlineParts = [];
      for (const orig of files) {
        try {
          const mimeType =
            orig.type || orig.mimeType || "application/octet-stream";
          if (orig?.arrayBuffer) {
            const ab = await orig.arrayBuffer();
            inlineParts.push({
              inlineData: {
                mimeType,
                data: Buffer.from(ab as any).toString("base64"),
              },
            });
          } else if (orig?.buffer) {
            inlineParts.push({
              inlineData: {
                mimeType,
                data: Buffer.from(orig.buffer).toString("base64"),
              },
            });
          } else if (orig?.path) {
            const { readFileSync } = await import("node:fs");
            inlineParts.push({
              inlineData: {
                mimeType,
                data: readFileSync(orig.path).toString("base64"),
              },
            });
          }
        } catch (e) {
          console.error("prepareMessagesForGemini: inline fallback failed", e);
        }
      }
      fileParts = inlineParts;
    }

    if (fileParts.length) {
      for (let i = arr.length - 1; i >= 0; i--) {
        if (arr[i].role === "user") {
          arr[i] = {
            ...arr[i],
            parts: [...(arr[i].parts || []), ...fileParts],
          };
          break;
        }
      }
    }
  }

  // 4) Tr·∫£ contents + systemInstruction (KH√îNG th√™m system v√†o contents)
  const contents = arr.map((m) => ({ role: m.role, parts: m.parts }));

  return {
    contents,
    systemInstruction: systemPrompt || undefined, // d√πng ·ªü b∆∞·ªõc getGenerativeModel ho·∫∑c generateContent
    uploadedFiles,
    attachedPartsCount: fileParts.length,
  };
}

async function processMessagesForOpenAI(messages: any[], files: any[]) {
  const processedMessages = [...messages];

  // Add files to the last user message for OpenAI
  if (files.length > 0 && processedMessages.length > 0) {
    const lastMessage = processedMessages[processedMessages.length - 1];
    if (lastMessage.role === "user") {
      // Convert content to array format if it's a string
      if (typeof lastMessage.content === "string") {
        lastMessage.content = [
          {
            type: "text",
            text: lastMessage.content,
          },
        ];
      }

      // Add supported files (images only for OpenAI)
      files.forEach((file: any) => {
        if (file.type.startsWith("image/")) {
          lastMessage.content.push({
            type: "image_url",
            image_url: {
              url: `data:${file.type};base64,${file.data}`,
            },
          });
        } else if (file.type === "application/pdf") {
          // For PDFs, add a text note since OpenAI doesn't support direct PDF processing
          lastMessage.content.push({
            type: "text",
            text: `[PDF File: ${file.name} - Note: OpenAI cannot directly process PDF files. Please extract text content manually.]`,
          });
        }
      });
    }
  }

  return processedMessages;
}

const CONTENT_SYSTEM: string = `
B·∫°n l√† Chatbot ORS. Nhi·ªám v·ª•: nh·∫≠n c√¢u h·ªèi c·ªßa user, sinh ra prompt ƒë∆°n gi·∫£n cho Gemini.
LU·ªíNG X·ª¨ L√ù:
User h·ªèi ‚Üí B·∫°n t·∫°o prompt ‚Üí Prompt g·ª≠i cho Gemini ‚Üí Gemini tr·∫£ l·ªùi
QUY T·∫ÆC SINH PROMPT:

1. N·∫øu user ch·ªâ ch√†o h·ªèi (hi, hello, xin ch√†o) ‚Üí Tr·∫£ l·ªùi: "Xin ch√†o! T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"

2. N·∫øu user h·ªèi th√¥ng tin ‚Üí Sinh prompt theo m·∫´u n√†y:
"Tr·∫£ l·ªùi c√¢u h·ªèi: [c√¢u h·ªèi user]

Y√™u c·∫ßu:
- Kh√¥ng d√πng k√Ω hi·ªáu **, kh√¥ng markdown
- C√≥ s·ªë li·ªáu b√°o c√°o n·∫øu c√≥
- Hi·ªÉn th·ªã √Ω ch√≠nh ƒë√∫ng tr·ªçng t√¢m, v√≠ d·ª•: ƒë√°nh s·ªë 1. 2. 3.
- M·ªói √Ω: 1 c√¢u t√≥m t·∫Øt + 1-2 c√¢u gi·∫£i th√≠ch
- N·∫øu coÃÅ chiÃâ d√¢ÃÉn ngu√¥ÃÄn, s·ªë li·ªáu th√¨ Cu·ªëi m·ªói √Ω ghi ngu·ªìn: <br /><strong>Ngu·ªìn:</strong> <a href='[link]' target='_blank'>[t√™n]</a>
Kh√¥ng c√≥ ngu·ªìn th√¨ kh√¥ng ghi.
- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát"

CH√ö √ù:
- [ch·ªß ƒë·ªÅ] = thay b·∫±ng lƒ©nh v·ª±c ph√π h·ª£p (t√†i ch√≠nh, gi√°o d·ª•c, y t·∫ø, ng√¢n h√†ng...)
- [c√¢u h·ªèi user] = copy y nguy√™n c√¢u h·ªèi c·ªßa user
- Ch·ªâ xu·∫•t prompt, kh√¥ng gi·∫£i th√≠ch g√¨ th√™m
`;

/**
 * @param contentText
 * @returns
 */
async function convertToPromptChatGPT(contentText: string): Promise<string> {
  const systemWithQuestion = CONTENT_SYSTEM.replace(
    "{user_question}",
    contentText.trim()
  );
  const promptGenerationMessages = [
    {
      role: "system",
      content: systemWithQuestion,
    },
    { role: "user", content: contentText },
  ];

  const chatgptResponse = await fetch(`${OPENAI_BASE_URL}/chat/completions`, {
    method: "POST",
    headers: {
      Authorization: `Bearer ${OPENAI_API_KEY}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      model: "gpt-4o-mini",
      messages: promptGenerationMessages,
      temperature: 0, // √≠t ‚Äús√°ng t√°c‚Äù
      top_p: 0, // ch·∫∑t ch·∫Ω h∆°n
      max_tokens: 400,
    }),
  });

  if (!chatgptResponse.ok) {
    const err = await chatgptResponse.json().catch(() => ({}));
    console.error("‚ùå ChatGPT API error:", err);
    return contentText;
  }

  const chatgptData = await chatgptResponse.json();
  const optimizedPrompt = String(
    chatgptData?.choices?.[0]?.message?.content || contentText
  );

  console.log("Prompt from ChatGPT ‚ú®:", optimizedPrompt);
  return optimizedPrompt;
}

async function processMessagesForGoogleSDK(
  messages: any[],
  files: any[],
  uploadedFiles: any[] = []
) {
  const processedContents = [];
  const prompts = [];

  for (const message of messages) {
    const role = message.role === "assistant" ? "model" : "user";
    const parts = [];

    // Add text content
    if (typeof message.content === "string") {
      console.log("Processing message content (string):", {
        role,
        contentLength: message.content.length,
      });
      const contentText = await convertToPromptChatGPT(message.content);
      parts.push({ text: contentText });
      prompts.push({
        input: { system: CONTENT_SYSTEM, user: message.content },
        output: contentText,
      });
    } else if (Array.isArray(message.content)) {
      console.log("Processing message content (array):", {
        role,
        contentItems: message.content.length,
        contentTypes: (message.content as any[]).map((c: any) => c.type),
      });

      message.content.forEach(async (content: any) => {
        // prompts.push({ input: null, output: null });
        if (content.type === "text") {
          // Convert to prompt
          // const contentText = await convertToPromptChatGPT(content.text);
          parts.push({ text: content.text });
        } else if (content.type === "image_url" && content.image_url?.url) {
          // Extract mime type and base64 data from data URL
          const dataUrl = content.image_url.url;
          const matches = (dataUrl as string).match(/^data:([^;]+);base64,(.+)$/);

          if (matches) {
            const [, mimeType, base64Data] = matches;
            console.log("Adding image part:", { mimeType });
            (parts as any).push({
              inlineData: {
                mimeType: mimeType,
                data: base64Data,
              },
            });
          }
        }
      });
    }

    if (parts.length > 0) {
      processedContents.push({
        role,
        parts,
      });
    }
  }

  if (uploadedFiles.length > 0 && processedContents.length > 0) {
    const lastContent = processedContents[processedContents.length - 1];
    if (lastContent.role === "user") {
      uploadedFiles.forEach((uploadedFile: any) => {
        console.log("Adding uploaded file part:", {
          name: uploadedFile.name,
          uri: uploadedFile.fileUri,
          mimeType: uploadedFile.mimeType,
        });

        (lastContent.parts as any).push({
          fileData: {
            mimeType: uploadedFile.mimeType,
            fileUri: uploadedFile.fileUri,
          },
        });
      });
    }
  }

  // Add inline images (not uploaded files)
  if (files.length > 0 && processedContents.length > 0) {
    const lastContent = processedContents[processedContents.length - 1];
    if (lastContent.role === "user") {
      files.forEach((file: any) => {
        // Only use inlineData for images (Office files are uploaded)
        if (file.type.startsWith("image/")) {
          console.log("Adding inline image:", { type: file.type });
          (lastContent.parts as any).push({
            inlineData: {
              mimeType: file.type,
              data: file.data,
            },
          });
        }
      });
    }
  }

  return { processedContents, prompts };
}